{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS009 - preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to walk through a single patient example. There are probably some patient-specific stuff in here that might change with other patients. Should be able to demonstrate the usage of different functions from the toolbox.\n",
    "\n",
    "1. Load raw data (.edf in this notebook) using mne\n",
    "\n",
    "2. Add in electrode information\n",
    "\n",
    "3. Notch filter line noise and cleaning out bad channels \n",
    "\n",
    "4. Re-reference the data (bipolar re-referencing)\n",
    "\n",
    "\n",
    "Must read guides: \n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S1053811922005559\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore, linregress\n",
    "import pandas as pd\n",
    "from mne.preprocessing.bads import _find_outliers\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/christinamaher/Documents/Github/LFPAnalysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LFPAnalysis import lfp_preprocess_utils, sync_utils, analysis_utils, nlx_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'MS009'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw data (.edf in this notebook) using mne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good idea to setup a sensible directory structure like below. Note that all my data lives on '/sc/arion' which is Minerva. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mne: https://mne.tools/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '/Users/christinamaher/Documents/Gem_Hunters/data/ieeg/' # this is the root directory for most un-archived data and results \n",
    "\n",
    "save_dir = f'{base_dir}/{subject}'  # save intermediate results in the 'work' directory\n",
    "    \n",
    "# I have saved most of my raw data in the 'projects directory'\n",
    "behav_dir = save_dir\n",
    "neural_dir = save_dir\n",
    "anat_dir = save_dir\n",
    "edf_files = glob(f'{neural_dir}/{subject}.edf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the electrophysiology data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne_data = mne.io.read_raw_edf(edf_files[0], preload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne_data.ch_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "plt.plot(mne_data._data[0,:4999])\n",
    "plt.title(\"Raw iEEG, electrode 0, samples 0-4999\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check the photodiode\n",
    "trig_ix = mne_data.ch_names.index('DC1') # either named DC1 or Research\n",
    "plt.plot(mne_data._data[trig_ix])\n",
    "plt.title(\"Photodiode\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save out the photodiode channel separately\n",
    "mne_data.save(f'{save_dir}/photodiode.fif', picks='DC1', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the photodiode channel\n",
    "mne_data.drop_channels(['DC1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add in electrode information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_name_dict = {x:x.replace(\" \", \"\").lower() for x in mne_data.ch_names}\n",
    "mne_data.rename_channels(new_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the electrode localization data and add it in\n",
    "csv_files = glob(f'{anat_dir}/{subject}_labels.csv')\n",
    "elec_locs = pd.read_csv(csv_files[0])\n",
    "\n",
    "# Sometimes there's extra columns with no entries: \n",
    "elec_locs = elec_locs[elec_locs.columns.drop(list(elec_locs.filter(regex='Unnamed')))]\n",
    "elec_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(elec_locs.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The electrode names read out of the edf file do not always match those \n",
    "in the pdf (used for localization). This could be error on the side of the tech who input the labels, \n",
    "or on the side of MNE reading the labels in. Usually there's a mixup between lowercase 'l' and capital 'I'.\n",
    "\n",
    "Sometimes, there's electrodes on the pdf that are NOT in the MNE data structure... let's identify those as well. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anat_names = list(elec_locs.label.str.lower())\n",
    "sum([ch not in mne_data.ch_names for ch in anat_names]) #if there are no missing channels, sum = 0. if sum >0, find the missing elecs\n",
    "print([ch for ch in mne_data.ch_names if ch not in anat_names ]) #print extra channels in mne_data.ch_names and make sure none of them are neural channels (will be EEG etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mne_names, unmatched_names, unmatched_seeg = lfp_preprocess_utils.match_elec_names(mne_data.ch_names, elec_locs.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmatched_seeg #make sure there are no unmatched names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_name_dict = {x:y for (x,y) in zip(mne_data.ch_names, new_mne_names)}\n",
    "new_name_dict #make sure this passes the eye test \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we retun a new list of channel names for the mne data structure as well as a list of channels in the localization csv which are not found in the mne structure. Make sure that unmatched_seeg does not factor into any referencing schemes later - it's not in the MNE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the mne data according to the localization data\n",
    "mne_data.rename_channels(new_name_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_seeg_names = [i for i in mne_data.ch_names if i.startswith('r')]\n",
    "left_seeg_names = [i for i in mne_data.ch_names if i.startswith('l')]\n",
    "print(f'We have a total of {len(left_seeg_names)} left sEEG and {len(right_seeg_names)} right sEEG electrodes')\n",
    "print(f'We have a total of {len(left_seeg_names) + len(right_seeg_names)} sEEG electrodes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sEEG_mapping_dict = {f'{x}':'seeg' for x in left_seeg_names+right_seeg_names}\n",
    "mne_data.set_channel_types(sEEG_mapping_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop random chans? \n",
    "drop_chans = list(set(mne_data.ch_names)^set(left_seeg_names+right_seeg_names))\n",
    "mne_data.drop_channels(drop_chans) #number of chans should = number of seegs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make montage (convert mm to m)\n",
    "\n",
    "montage = mne.channels.make_dig_montage(ch_pos=dict(zip(elec_locs.label, \n",
    "                                                        elec_locs[['mni_x', 'mni_y', 'mni_z']].to_numpy(dtype=float)/1000)),\n",
    "                                        coord_frame='mni_tal')\n",
    "\n",
    "mne_data.set_montage(montage, match_case=False, on_missing='warn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notch filter line noise and resample to 500 hz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to remove the line noise (60 Hz and harmonics in US data, 50 Hz and harmonics in EU data). \n",
    "\n",
    "To do so, we use a band-stop filter that removes a narrow band of frequencies. \n",
    "\n",
    "Maybe eventually we don't want to use filters, especially if interested in ERPs: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6456018/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne_data.info['line_freq'] = 60\n",
    "# Notch out 60 Hz noise and harmonics \n",
    "mne_data.notch_filter(freqs=(60, 120, 180, 240))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all patients should be resampled to 500 Hz\n",
    "resample_sr = 500\n",
    "mne_data.resample(sfreq=resample_sr, npad='auto', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal Cleaning \n",
    "Methods:\n",
    "- Use manual bad channel detection for **bipolar** referencing\n",
    "    - bipolar referencing completely ignores channels labeled as 'bad' so do not remove unless absolutely necessary\n",
    "    - remove ch if there is massive artifact in a channel not in the rest of probe \n",
    "    - if entire probe shows same artifact try to keep\n",
    "- Remove additional channels as needed\n",
    "- This process is iterative depending on how TFRs look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_names = list(elec_locs.label.str.lower())\n",
    "pattern = '[0-9]'\n",
    "ch_names_no_num = [re.sub(pattern, '', i) for i in ch_names]\n",
    "probe_names = np.unique(ch_names_no_num)\n",
    "probe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_ch_counts = {} #need this to select channel number for visualization\n",
    "for p in probe_names:\n",
    "    c = ch_names_no_num.count(p)\n",
    "    probe_ch_counts[p] = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_ch_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_preprocess_utils.detect_bad_elecs(mne_data, sEEG_mapping_dict) # likely inaccurate, just use it to get a general idea of problematic channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %matplotlib notebook\n",
    "fig = mne_data.plot(start=0, duration=1000, n_channels=40, scalings=mne_data._data.max()/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne_data.info['bads']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne_data.info #sanity check that bads info saved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bipolar re-referencing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're like me, you find the concept of re-referencing somewhat confusing. Isn't the data recorded relative to a ground and reference in the EMU (https://ahleighton.github.io/OE-ephys-course/EEA/theoryday3.html)? \n",
    "\n",
    "It is, but we do digital re-referencing of the recorded signal to clean up any remaining shared noise. \n",
    "\n",
    "**Re-referencing should be an EXTREMELY conscious choice as it changes the LFP signal dramatically!** In our case, we choose to do local white-matter re-referencing because electrodes in white matter should be fairly stable (low-variance) and not contain local, slow oscillations of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the localization data to determine the gray vs. white matter electrodes. \n",
    "Then, let's re-reference each gray matter electrode to the closest and most low-amplitude white matter electrode. \n",
    "\n",
    "Make sure 'bad' electrodes are not used in the re-referencing. Same with unmatched seeg electrodes (not present in the mne data structure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-reference neural data\n",
    "mne_data_bp_reref = lfp_preprocess_utils.ref_mne(mne_data=mne_data, \n",
    "                                              elec_path=anat_file, \n",
    "                                              method='bipolar', \n",
    "                                              site='MSSM')\n",
    "mne_data_bp_reref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne_data_bp_reref.ch_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine bp re-referenced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " %matplotlib notebook\n",
    "fig = mne_data_bp_reref.plot(start=0, duration=1000, n_channels=40, scalings=mne_data_bp_reref._data.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne_data_bp_reref.compute_psd().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne_data_bp_reref.save(f'{neural_dir}{subj_id}/bp_ref_ieeg.fif',overwrite=True)\n",
    "mne_data.save(f'{neural_dir}{subj_id}/raw_ieeg.fif',overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epoching + TFRs\n",
    "- Check whether data is ready to be analyzed - if this step shows noise then steps above should be repeated\n",
    "- Align photodiode to behavior \n",
    "- Epoch data (and mark bad epochs)\n",
    "- Baseline data\n",
    "- Visualize TFRs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove mne_data from environment to save memory \n",
    "del mne_data, mne_data_bp_reref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photodiode alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the photodiode and resample to match the neural data\n",
    "photodiode = mne.io.read_raw_fif(f'{base_dir}/photodiode.fif', preload=True)\n",
    "resample_sr = 500\n",
    "photodiode.resample(sfreq=resample_sr, npad='auto', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load behavior data and save timestamp(s) of interest as variable \n",
    "behav_df = pd.read_csv(f'{behav_dir}/{subject}_clean.csv')\n",
    "\n",
    "choice_ts = behav_df['choice_ts'].copy()\n",
    "\n",
    "# add column of ITI ts\n",
    "behav_df['iti_ts'] = behav_df['choice_ts'].copy() + 1500\n",
    "iti_ts = behav_df['iti_ts'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot photodiode and choice timestamps before alignment\n",
    "%matplotlib qt\n",
    "plt.plot(photodiode._data[0])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"V\")\n",
    "plt.title(\"Photodiode\")\n",
    "\n",
    "zeros = np.array([0.05] * len(choice_ts))\n",
    "x_ts = choice_ts\n",
    "y_ts = zeros.T\n",
    "plt.scatter(x_ts,y_ts,color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample time-frequency data as an example\n",
    "time_frequency_data = photodiode._data[0]\n",
    "\n",
    "# Define the threshold values\n",
    "threshold_min = -0.15\n",
    "\n",
    "photodiode_deflected = []\n",
    "\n",
    "# Detect sequential peaks\n",
    "for value in time_frequency_data:\n",
    "    if value > threshold_min:\n",
    "        photodiode_deflected.append(1)\n",
    "    elif value < threshold_min:\n",
    "        photodiode_deflected.append(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_sequential_values(arr):\n",
    "    if not arr:\n",
    "        return []\n",
    "\n",
    "    result = []\n",
    "    current_sum = arr[0]\n",
    "\n",
    "    for i in range(1, len(arr)):\n",
    "        if arr[i] == arr[i - 1]:\n",
    "            current_sum += arr[i]\n",
    "        else:\n",
    "            result.append(current_sum)\n",
    "            current_sum = arr[i]\n",
    "\n",
    "    # Append the last calculated sum\n",
    "    result.append(current_sum)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = sum_sequential_values(photodiode_deflected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_peak_indices(lengths):\n",
    "    photodiode_indices = []\n",
    "    \n",
    "    for l in lengths:\n",
    "        if l < 0:\n",
    "            l_temp = l * -1 # flip sign\n",
    "            drop_numbers = np.ones(l_temp, dtype=int)\n",
    "            photodiode_indices.append(drop_numbers)\n",
    "        elif (l > 800) | (l < 700):\n",
    "            drop_numbers = np.ones(l, dtype=int)\n",
    "            photodiode_indices.append(drop_numbers)\n",
    "        elif (1 < 800) & (l > 700): \n",
    "            keep_numbers = np.zeros(l, dtype=int)\n",
    "            photodiode_indices.append(keep_numbers)\n",
    "        else:\n",
    "            print(\"error\")\n",
    "    \n",
    "    return np.concatenate(photodiode_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photodiode_indices = assign_peak_indices(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_diode = photodiode._data[0].copy()\n",
    "peaks_to_exclude = [index for index, value in enumerate(photodiode_indices) if value == 1]\n",
    "new_diode[peaks_to_exclude] = np.min(photodiode._data[0]) # make these all super small\n",
    "\n",
    "photodiode_final = photodiode.copy()\n",
    "photodiode_final._data[0] = new_diode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot photodiode\n",
    "plt.plot(photodiode_final._data[0])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"V\")\n",
    "plt.title(\"Photodiode\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope, offset = sync_utils.synchronize_data(choice_ts, \n",
    "                                            photodiode_final, \n",
    "                                            smoothSize=11, windSize=10, height=0.7)\n",
    "print(slope,offset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize updated choice ts \n",
    "choice_ts = choice_ts * slope + offset\n",
    "\n",
    "%matplotlib qt\n",
    "plt.plot(photodiode_final._data[0])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"V\")\n",
    "plt.title(\"Photodiode\")\n",
    "\n",
    "zeros = np.array([-0.01] * len(choice_ts))\n",
    "x_ts = choice_ts * 500\n",
    "y_ts = zeros.T\n",
    "plt.scatter(x_ts,y_ts,color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize updated iti \n",
    "iti_ts = iti_ts * slope + offset\n",
    "\n",
    "%matplotlib qt\n",
    "plt.plot(photodiode_final._data[0])\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"V\")\n",
    "plt.title(\"Photodiode\")\n",
    "\n",
    "zeros = np.array([-0.01] * len(iti_ts))\n",
    "x_ts = iti_ts * 500\n",
    "y_ts = zeros.T\n",
    "plt.scatter(x_ts,y_ts,color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav_df['choice_ts'] = [(x*slope + offset) for x in behav_df['choice_ts']]\n",
    "behav_df['iti_ts'] = [(x*slope + offset) for x in behav_df['iti_ts']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epoch Data\n",
    "- Epoch neural data into trial epochs \n",
    "- Add behavioral data to epochs metadata\n",
    "- Save epochs\n",
    "- Baseline + decompose data into TFRs\n",
    "- Plot + save TFRs (examine quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IED removal requires that we set some parameters for IED detection. \n",
    "# 1. peak_thresh: how many stds should they exceed the baseline by? \n",
    "# 2. closeness_thresh: how close should they be allowed to be (in sec) to other candidate IEDs? \n",
    "# 3. width_thresh: how wide should they have to be (in sec)?\n",
    "\n",
    "# Defaults:\n",
    "IED_args = {'peak_thresh':4,\n",
    "           'closeness_thresh':0.25, \n",
    "           'width_thresh':0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with your event name (matching your dataframe), and the time-window for the event\n",
    "evs = {'choice_ts': [-1.5, 1.5], \n",
    "       'iti_ts': [0.0, 0.5]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_all_evs = {f'{x}': np.nan for x in evs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in evs.keys():\n",
    "    # Make the epochs. \n",
    "    ev_epochs = lfp_preprocess_utils.make_epochs(load_path=f'{neural_dir}{subj_id}/bp_ref_ieeg.fif', \n",
    "                 slope=slope, offset=offset,\n",
    "                 behav_name=event, behav_times=behav_df[event].values, \n",
    "                 ev_start_s=evs[event][0], ev_end_s=evs[event][1], buf_s = 1.0, IED_args=IED_args) #1.0 buf unsaved\n",
    "\n",
    "    epochs_all_evs[event] = ev_epochs\n",
    "\n",
    "epochs_all_evs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add behavioral data to metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select params of interest \n",
    "behav_params = [\"learned\",\"reward\",\n",
    "               \"correct\",\"rd\",\"tf\",\"condition\",\n",
    "                \"phiEVcombo\",\"phiRPEcombo\",\n",
    "                \"phiEVrd\",\"phiRPErd\",\n",
    "               \"chosen_f_rd\",\"chosen_f_ird\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in evs.keys():\n",
    "\n",
    "    event_metadata = epochs_all_evs[event].metadata.copy()\n",
    "    \n",
    "    #independent vars\n",
    "    for param in behav_params: \n",
    "        event_metadata[param] = behav_df[param].tolist()\n",
    "\n",
    "    epochs_all_evs[event].metadata = event_metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_all_evs[event].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save raw epoched data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in evs.keys():\n",
    "    epochs_all_evs[event].save(f'{neural_dir}{subj_id}/bp_epoch_{event}.fif', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline + Decompose into TFRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly define a list of analysis events and the baseline event. Should correspond to the dict\n",
    "analysis_evs = ['choice_ts']\n",
    "baseline_ev = 'iti_ts'\n",
    "evs = {evs = {'choice_ts': [-1.5, 1.5], \n",
    "       'iti_ts': [0.0, 0.5]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some spectrogram parameters \n",
    "freqs = np.logspace(*np.log10([2, 200]), num=30)\n",
    "n_cycles = np.floor(np.logspace(*np.log10([3, 10]), num=30))\n",
    "sr = 500.0 \n",
    "buf = 1.0\n",
    "buf_ix = int(buf*sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_all_baseline = {} #removed unecessary 'decisiononset' empty key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline epoch - \n",
    "event = 'iti_ts'\n",
    "epochs = epochs_all_evs[event]\n",
    "\n",
    "good_chans = [x for x in epochs.ch_names if x not in epochs.info['bads']]\n",
    "picks = [x for x in good_chans]\n",
    "\n",
    "pow_struct = np.nan * np.ones([epochs._data.shape[0], \n",
    "                       epochs._data.shape[1], len(freqs), \n",
    "                       epochs._data.shape[-1]])\n",
    "\n",
    "for ch_ix in np.arange(epochs._data.shape[1]): \n",
    "    ch_data = epochs._data[:, ch_ix:ch_ix+1, :]\n",
    "    bad_epochs  = np.where(epochs.metadata[epochs.ch_names[ch_ix]].notnull())[0]\n",
    "    good_epochs = np.delete(np.arange(ch_data.shape[0]), bad_epochs)\n",
    "    ch_data = np.delete(ch_data, bad_epochs, axis=0)\n",
    "    ch_pow = mne.time_frequency.tfr_array_morlet(ch_data, sfreq=epochs.info['sfreq'], \n",
    "                                        freqs=freqs, n_cycles=n_cycles, zero_mean=False, \n",
    "                                        use_fft=True, output='power', n_jobs=1)\n",
    "\n",
    "    pow_struct[good_epochs, ch_ix, :, :] = ch_pow[:, 0, :, :]\n",
    "\n",
    "temp_pow = mne.time_frequency.EpochsTFR(epochs.info, pow_struct, \n",
    "                                        epochs.times, freqs)\n",
    "temp_pow.crop(tmin=evs[event][0], tmax=evs[event][1])\n",
    "\n",
    "epochs_all_baseline[event] = temp_pow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_all_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_epochs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = 'choice_ts'\n",
    "\n",
    "epochs = epochs_all_evs[event]\n",
    "\n",
    "# Let's make sure we only do this for good channels\n",
    "good_chans = [x for x in epochs.ch_names if x not in epochs.info['bads']]\n",
    "picks = [x for x in good_chans]\n",
    "\n",
    "pow_struct = np.nan * np.ones([epochs._data.shape[0], \n",
    "                       epochs._data.shape[1], len(freqs), \n",
    "                       epochs._data.shape[-1]])\n",
    "\n",
    "for ch_ix in np.arange(epochs._data.shape[1]): \n",
    "    ch_data = epochs._data[:, ch_ix:ch_ix+1, :]\n",
    "    bad_epochs  = np.where(epochs.metadata[epochs.ch_names[ch_ix]].notnull())[0] \n",
    "    good_epochs = np.delete(np.arange(ch_data.shape[0]), bad_epochs)\n",
    "    ch_data = np.delete(ch_data, bad_epochs, axis=0) #this is where bad epochs for ch are deleted!!\n",
    "    ch_pow = mne.time_frequency.tfr_array_morlet(ch_data, sfreq=epochs.info['sfreq'], \n",
    "                                        freqs=freqs, n_cycles=n_cycles, zero_mean=False, \n",
    "                                        use_fft=True, output='power', n_jobs=1)\n",
    "\n",
    "    pow_struct[good_epochs, ch_ix, :, :] = ch_pow[:, 0, :, :]\n",
    "\n",
    "temp_pow = mne.time_frequency.EpochsTFR(epochs.info, pow_struct, \n",
    "                                        epochs.times, freqs)\n",
    "\n",
    "temp_pow.crop(tmin=evs[event][0], tmax=evs[event][1])\n",
    "\n",
    "\n",
    "baseline_corrected_power = lfp_preprocess_utils.baseline_trialwise_TFR(data=temp_pow.data, \n",
    "                                                  baseline_mne=epochs_all_baseline['iti_ts'], \n",
    "                                                  mode='zscore', \n",
    "                                                  trialwise=False, ## make sure this is FALSE! More robust baselining method if set to TRUE.\n",
    "                                                  baseline_only=True)\n",
    "\n",
    "\n",
    "zpow = mne.time_frequency.EpochsTFR(epochs.info, baseline_corrected_power, \n",
    "                                temp_pow.times, freqs)\n",
    "\n",
    "zpow.metadata = epochs_all_evs[event].metadata\n",
    "\n",
    "power_epochs[event] = zpow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_epochs['choice_ts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge h5io must install to save tfr data\n",
    "power_epochs['choice_ts'].save(f'{save_dir}/bp_pow_epochs-tfr.h5', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot TFRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####add directory for TFRs\n",
    "tfr_dir = f'{base_dir}/{subject}/tfr/'\n",
    "os.makedirs(tfr_dir,exist_ok = True) #added so you don't have to manually make subject folders in clean_data\n",
    "date = datetime.date.today().strftime('%m%d%Y')\n",
    "\n",
    "print(date)\n",
    "# mne_data_bp_reref = mne.io.read_raw_fif(f'{neural_dir}{subj_id}/bp_ref_ieeg.fif',preload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = 'choice_ts'\n",
    "yticks = [4, 12, 30, 60, 90, 120, 150, 180, 200]\n",
    "good_ch = [x for x in power_epochs[event].ch_names if '-' in x]\n",
    "save_path = tfr_dir\n",
    "\n",
    "print(offset)\n",
    "\n",
    "\n",
    "for ch in good_ch:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "    times = power_epochs[event].times\n",
    "    plot_data = np.nanmean(np.nanmean(power_epochs[event].copy().pick_channels([ch]).data, axis=0), axis=0)\n",
    "\n",
    "    im = ax.imshow(plot_data,\n",
    "            extent=[times[0], times[-1], freqs[0], freqs[-1]], interpolation='Bicubic',\n",
    "            aspect='auto', origin='lower', cmap='RdBu_r',vmin = -np.nanmax(np.abs(plot_data)), vmax = np.nanmax(np.abs(plot_data)))\n",
    "    ax.set(yticks=yticks, xlabel='Time (s)', ylabel='Frequency',title=f'{ch} Encoding')\n",
    "    ax.yaxis.set_tick_params(labelsize=8)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    plt.savefig(f'{save_path}/{ch}_{date}_bp_ref.png', format='png', metadata=None,\n",
    "    bbox_inches=None, pad_inches=0.1,\n",
    "    facecolor='auto', edgecolor='auto',\n",
    "    backend=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
