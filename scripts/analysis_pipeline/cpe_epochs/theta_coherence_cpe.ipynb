{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63c15ef6",
   "metadata": {},
   "source": [
    "# SWB CPE Connectivity Analysis: Theta Coherence\n",
    "\n",
    "Created: 10/07/2024 \\\n",
    "Updated: 11/25/2024 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b06ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore, linregress, ttest_ind, ttest_rel, ttest_1samp, pearsonr, spearmanr\n",
    "import pandas as pd\n",
    "from mne.preprocessing.bads import _find_outliers\n",
    "import os \n",
    "import joblib\n",
    "import re\n",
    "import datetime\n",
    "import scipy\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.regression.mixed_linear_model import MixedLM \n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "import itertools\n",
    "import time \n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "\n",
    "\n",
    "import mne_connectivity\n",
    "from mne_connectivity import phase_slope_index, seed_target_indices, spectral_connectivity_epochs\n",
    "# import fooof\n",
    "# Import plotting functions\n",
    "# from fooof.plts.spectra import plot_spectrum, plot_spectra\n",
    "# # Import the FOOOF object\n",
    "# from fooof import FOOOF\n",
    "# from fooof import FOOOFGroup\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from joblib import delayed, Parallel\n",
    "from statsmodels.stats import multitest\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# print('\\n'.join(f'{m.__name__}=={m.__version__}' for m in globals().values() if getattr(m, '__version__', None)))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d63e14b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "band = 'theta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e13b8c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11262024\n"
     ]
    }
   ],
   "source": [
    "# Specify root directory for un-archived data and results \n",
    "base_dir   = '/sc/arion/projects/guLab/Alie/SWB/'\n",
    "neural_dir = f'{base_dir}ephys_analysis/data/'\n",
    "behav_dir  = f'{base_dir}ephys_analysis/behav/behav_data/'\n",
    "save_dir   = f'{base_dir}ephys_analysis/results/connectivity/coherence/{band}/'\n",
    "os.makedirs(save_dir,exist_ok=True)\n",
    "\n",
    "script_dir = '/hpc/users/finka03/swb_ephys_analysis/scripts/'\n",
    "\n",
    "date = datetime.date.today().strftime('%m%d%Y')\n",
    "print(date)\n",
    "\n",
    "# anat_dir   = f'{base_dir}ephys_analysis/recon_labels/'\n",
    "# behav_dir  = f'{base_dir}swb_behav_models/data/behavior_preprocessed/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ebfd527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(f'{base_dir}ephys_analysis/LFPAnalysis/')\n",
    "\n",
    "from LFPAnalysis import oscillation_utils\n",
    "\n",
    "sys.path.append(f'{script_dir}analysis_notebooks/')\n",
    "\n",
    "from ieeg_tools import *\n",
    "\n",
    "sys.path.append(f'{script_dir}behav/')\n",
    "\n",
    "from behav_utils import *\n",
    "from swb_subj_behav import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9316f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_ids = list(pd.read_excel(f'{base_dir}ephys_analysis/subj_info/SWB_subjects.xlsx', \n",
    "                              sheet_name='Usable_Subjects', usecols=[0]).PatientID)\n",
    "n_subj = len(subj_ids)\n",
    "# subj_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d29d296-a32e-45b7-a2e9-4605d507fb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy==1.26.4\n",
      "mne==1.8.0\n",
      "seaborn==0.13.2\n",
      "pandas==2.2.2\n",
      "joblib==1.3.1\n",
      "re==2.2.1\n",
      "scipy==1.14.1\n",
      "statsmodels.api==0.14.2\n",
      "mne_connectivity==0.7.0\n",
      "patsy==0.5.6\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(f'{m.__name__}=={m.__version__}' for m in globals().values() if getattr(m, '__version__', None)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fb5f14-a38b-477e-8ebd-5f0b67a60f96",
   "metadata": {},
   "source": [
    "# Load Behav + Elec ROI Data\n",
    "- all_behav from updated task_dfs and behav_utils formatting\n",
    "- roi_reref_labels same as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30605ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all_behav = pd.read_csv(f'{behav_dir}all_behav.csv') ## this isn't normalized yet \n",
    "# raw_behav = [pd.read_csv(f'{behav_dir}{subj_id}_task_df.csv') for subj_id in subj_ids]\n",
    "# all_behav,drops_data = format_all_behav(raw_behav,return_drops=True)\n",
    "\n",
    "# all_behav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6342b194-d8ac-40ef-b0a9-f1d9ca362306",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_behav = [pd.read_csv(f'{behav_dir}{subj_id}_task_df.csv') for subj_id in subj_ids]\n",
    "temp_behav,beh_drops = format_all_behav(raw_behav,return_drops=True,norm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c819c072-47b1-44a8-b0e6-2c1a106c837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_orthog_rpe  = []\n",
    "all_rpe_cpe_rsq = []\n",
    "behav_list = []\n",
    "\n",
    "for ix, subj_id in enumerate(subj_ids): \n",
    "#     subj_df = raw_behav[ix]\n",
    "    subj_df = temp_behav[temp_behav.subj_id==subj_id].reset_index(drop=True)\n",
    "    # find indices where cpe is not nan\n",
    "    notnan_idx = [ix for ix,cpe in enumerate(subj_df.cpe) if not np.isnan(cpe)]\n",
    "    # initialize save vector\n",
    "    subj_df['orthog_rpe'] = np.nan\n",
    "    # run ols regression rpe ~ cpe\n",
    "    rpe_cpe = sm.OLS(subj_df.rpe,sm.add_constant(subj_df.cpe),missing='drop').fit()\n",
    "    all_rpe_cpe_rsq.append(rpe_cpe.rsquared)\n",
    "    subj_df['orthog_rpe'][notnan_idx] = rpe_cpe.resid\n",
    "    all_orthog_rpe.extend(subj_df['orthog_rpe'])\n",
    "    behav_list.append(subj_df)\n",
    "    del rpe_cpe \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8645a5b0-e94e-4930-aeb0-b27edfb99738",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_behav,beh_drops = format_all_behav(behav_list,return_drops=True,norm=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7b52d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_behav.to_csv(f'{save_dir}all_behav_{band}_coh_{date}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbcf4065-79bd-4658-82cf-061eb66e476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_reref_labels_master_df = pd.read_csv(\n",
    "    glob(f'{base_dir}ephys_analysis/results/roi_info/roi_reref_labels_master.csv')[0]).drop(columns=['Unnamed: 0'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9019470-f983-46fd-b09b-ce2ef462cafb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'theta'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "band"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d880360",
   "metadata": {},
   "source": [
    "# Connectivity Computations : Theta Coherence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5afc6fc-ed52-47d2-9e05-7f34ecdce18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define connectivity analysis parameters:\n",
    "\n",
    "# spectral parameters - wavelet freqs, wavelet cycles, freq band ranges\n",
    "freqs = np.logspace(*np.log10([2, 200]), num=30)\n",
    "n_cycles = np.floor(np.logspace(*np.log10([3, 10]), num=30))\n",
    "\n",
    "freq_dict = {'theta':[4, 8], \n",
    "             'alpha':[8, 13],\n",
    "             'beta': [13, 30], \n",
    "             'gamma': [30,70],\n",
    "             'hfa': [70, 200]}\n",
    "\n",
    "# analysis parameters - connectivity metric, conn freq band, num of surrogates for permutations, buffer time in ms\n",
    "metric   = 'coh'\n",
    "band     = 'theta' # set band(s) of interest for analysis\n",
    "n_surr   = 500\n",
    "buf_ms   = 1000\n",
    "\n",
    "# data info - analysis epoch + rois for pairwise coh\n",
    "epoch_id = 'CpeOnset' \n",
    "# rois\n",
    "coh_rois = ['acc','ains','ofc','dlpfc','vlpfc','amy','dmpfc','hpc']\n",
    "\n",
    "# dict of subj_ids with elecs in roi \n",
    "roi_subj_ids = {f'{roi}':roi_reref_labels_master_df.subj_id[\n",
    "    roi_reref_labels_master_df.roi == roi].unique().tolist() \n",
    "                for roi in coh_rois}\n",
    "# # make unique list of pairs [[pair1,pair2],..] without hard coding \n",
    "# pairs = [list(tup) for tup in list(itertools.combinations(coh_rois,2))]\n",
    "# pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43bde499-1c40-473b-8917-970547d4de96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['acc', 'ains'],\n",
       " ['acc', 'ofc'],\n",
       " ['acc', 'dlpfc'],\n",
       " ['acc', 'vlpfc'],\n",
       " ['acc', 'amy'],\n",
       " ['acc', 'dmpfc'],\n",
       " ['acc', 'hpc'],\n",
       " ['ains', 'ofc'],\n",
       " ['ains', 'dlpfc'],\n",
       " ['ains', 'vlpfc'],\n",
       " ['ains', 'amy'],\n",
       " ['ains', 'dmpfc'],\n",
       " ['ains', 'hpc'],\n",
       " ['ofc', 'dlpfc'],\n",
       " ['ofc', 'vlpfc'],\n",
       " ['ofc', 'amy'],\n",
       " ['ofc', 'dmpfc'],\n",
       " ['ofc', 'hpc'],\n",
       " ['dlpfc', 'vlpfc'],\n",
       " ['dlpfc', 'amy'],\n",
       " ['dlpfc', 'dmpfc'],\n",
       " ['dlpfc', 'hpc'],\n",
       " ['vlpfc', 'amy'],\n",
       " ['vlpfc', 'dmpfc'],\n",
       " ['vlpfc', 'hpc'],\n",
       " ['amy', 'dmpfc'],\n",
       " ['amy', 'hpc'],\n",
       " ['dmpfc', 'hpc']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_combos = [list(tup) for tup in list(itertools.combinations(coh_rois,2))]\n",
    "all_combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e204bc39-fe56-4603-8ca7-6f547925a226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dmpfc', 'hpc'],\n",
       " ['dlpfc', 'amy'],\n",
       " ['ains', 'amy'],\n",
       " ['acc', 'amy'],\n",
       " ['acc', 'hpc'],\n",
       " ['dlpfc', 'dmpfc'],\n",
       " ['ofc', 'dlpfc'],\n",
       " ['ofc', 'vlpfc'],\n",
       " ['ains', 'hpc'],\n",
       " ['acc', 'ains'],\n",
       " ['ofc', 'hpc'],\n",
       " ['acc', 'dlpfc'],\n",
       " ['amy', 'hpc'],\n",
       " ['ofc', 'amy'],\n",
       " ['ains', 'dmpfc'],\n",
       " ['acc', 'dmpfc'],\n",
       " ['dlpfc', 'hpc'],\n",
       " ['ains', 'dlpfc'],\n",
       " ['dlpfc', 'vlpfc'],\n",
       " ['vlpfc', 'hpc'],\n",
       " ['ains', 'vlpfc'],\n",
       " ['acc', 'vlpfc'],\n",
       " ['vlpfc', 'dmpfc'],\n",
       " ['amy', 'dmpfc'],\n",
       " ['ains', 'ofc'],\n",
       " ['acc', 'ofc'],\n",
       " ['vlpfc', 'amy'],\n",
       " ['ofc', 'dmpfc']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finished_pairs = [file.split('_')[:2] for file in os.listdir(f'{save_dir}pair_data/')]\n",
    "finished_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56ff1cc9-2632-43fc-929b-9828b75a4452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## unique list of all pairs that have any single subjects saved \n",
    "all_pair_files = [file.split('_')[1:3] for file in os.listdir(f'{save_dir}single_subj/') if file.split('_')[-1] == 'df.csv']\n",
    "all_pair_files\n",
    "\n",
    "pair_subj_data = []\n",
    "for pair in all_pair_files:\n",
    "    if pair not in pair_subj_data:\n",
    "        pair_subj_data.append(pair)\n",
    "pair_subj_data\n",
    "\n",
    "# # find which pairs weren't completed \n",
    "incomplete_pairs = [pair for pair in pair_subj_data if pair not in finished_pairs]\n",
    "incomplete_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88509329-bce6-497f-9500-92f248b629cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_subj_dict   = {}\n",
    "incomplete_subj_dict = {}\n",
    "\n",
    "for pair in incomplete_pairs:\n",
    "    source_region = pair[0]\n",
    "    target_region = pair[1]    \n",
    "    # unique pair id roi1_roi2\n",
    "    pair_id = '_'.join([source_region,target_region])\n",
    "\n",
    "    # find subj with elecs in each roi \n",
    "    source_subj = roi_subj_ids[source_region]\n",
    "    target_subj = roi_subj_ids[target_region]\n",
    "    \n",
    "    # # find subj with elecs in both rois\n",
    "    pair_subj = list(set(source_subj).intersection(target_subj))    \n",
    "    # # find completed subj files\n",
    "    complete_subj_files = glob(f'{save_dir}single_subj/*{pair_id}_coh_{band}_df.csv')\n",
    "    complete_subj = [file.split('/')[-1].split('_')[0] for file in complete_subj_files]\n",
    "    complete_subj_dict[pair_id] = complete_subj\n",
    "\n",
    "    # # find subj from pair_subj with no saved data\n",
    "    incomplete_subj = list(np.setdiff1d(pair_subj, complete_subj))\n",
    "    incomplete_subj_dict[pair_id] = incomplete_subj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "834ed7cb-447e-4d75-8185-d7c70a0e50b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_pairs = [pair for pair in all_combos if (pair not in finished_pairs) & (pair not in incomplete_pairs)]\n",
    "run_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39af1744-24c5-424b-b062-1bf374bd6918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add incomplete pair to beginning of list to run first \n",
    "pairs = incomplete_pairs + run_pairs\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "436d04d6-0d40-47f3-bfb7-b312d32afed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairs = [['ains', 'dmpfc'],\n",
    "#          ['ofc', 'amy'],\n",
    "#          ['ofc', 'dmpfc'],\n",
    "#          ['amy', 'dmpfc'],\n",
    "#          ['ofc', 'vlpfc'],\n",
    "#          ['vlpfc', 'amy'],\n",
    "#          ['acc', 'hpc'],\n",
    "#          ['ains', 'hpc'],\n",
    "#          ['ofc', 'hpc'],\n",
    "#          ['dlpfc', 'hpc'],\n",
    "#          ['amy', 'hpc'],\n",
    "#          ['dmpfc', 'hpc'],\n",
    "#          ['vlpfc', 'hpc'],\n",
    "#          ['vlpfc', 'dmpfc'],\n",
    "#          ['dlpfc', 'vlpfc'],\n",
    "#          ['dlpfc', 'dmpfc']\n",
    "#         ]\n",
    "# pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b34132b-7f0b-4d97-ab52-420b9a4eac4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /sc/arion/projects/guLab/Alie/SWB/ephys_analysis/data/MS035/MS035_conn_epochs_CpeOnset.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "   Processing epoch 148 / 150 ...\n",
      "   Processing epoch 143 / 150 ...\n",
      "   Processing epoch 143 / 150 ...\n"
     ]
    }
   ],
   "source": [
    "######## to compute coh after notebook ends in middle of roi pair\n",
    "\n",
    "for pair in pairs:\n",
    "    \n",
    "    source_region = pair[0]\n",
    "    target_region = pair[1]    \n",
    "    \n",
    "    # unique pair id roi1_roi2\n",
    "    pair_id = '_'.join([source_region,target_region])\n",
    "    \n",
    "    # find subj with elecs in each roi \n",
    "    source_subj = roi_subj_ids[source_region]\n",
    "    target_subj = roi_subj_ids[target_region]\n",
    "    # find subj with elecs in both rois\n",
    "    pair_subj = list(set(source_subj).intersection(target_subj))    \n",
    "    # save pair subj list \n",
    "\n",
    "    if pair in incomplete_pairs:\n",
    "        complete_subj   = complete_subj_dict[pair_id]\n",
    "        incomplete_subj = incomplete_subj_dict[pair_id]\n",
    "    \n",
    "    # initialize the storage list \n",
    "    all_subj_pair_df = []\n",
    "    \n",
    "    # iterate through pair subjects\n",
    "    for subj_id in pair_subj:\n",
    "        \n",
    "        if pair in incomplete_pairs:\n",
    "            if subj_id in complete_subj:\n",
    "                continue\n",
    "                \n",
    "        # load & format rereferenced epoch data \n",
    "        subj_epochs = mne.read_epochs(f'{neural_dir}{subj_id}/{subj_id}_conn_epochs_{epoch_id}.fif', preload=True)\n",
    "        subj_elecs  = subj_epochs.ch_names\n",
    "        \n",
    "        # construct the seed-to-target mapping based on subject's roi coverage \n",
    "        elec_roi_df = roi_reref_labels_master_df[roi_reref_labels_master_df.subj_id==subj_id].reset_index(drop=True)\n",
    "        # get ch names of subj elecs in roi \n",
    "        source_ch_names  = elec_roi_df.reref_ch_names[np.where(elec_roi_df.roi == source_region)[0]].tolist()\n",
    "        target_ch_names  = elec_roi_df.reref_ch_names[np.where(elec_roi_df.roi == target_region)[0]].tolist()\n",
    "        # get idx of ch in subj_elecs list (will correspond to idx in epochs array)\n",
    "        source_elec_idx = [subj_elecs.index(elec) for elec in source_ch_names]\n",
    "        target_elec_idx = [subj_elecs.index(elec) for elec in target_ch_names]\n",
    "        # make seed to target indices using mne function \n",
    "        seed_to_target = seed_target_indices(\n",
    "                        source_elec_idx,\n",
    "                        target_elec_idx)\n",
    "        \n",
    "        # elec name for every elec pair \n",
    "        subj_pair_ch = list(map(lambda x,y: '_'.join([x,y]), \n",
    "                                  [subj_elecs[idx] for idx in  seed_to_target[0]], \n",
    "                                  [subj_elecs[idx] for idx in  seed_to_target[1]]))\n",
    "        # unique elec name for every elec pair \n",
    "        unique_ch_pair = list(map(lambda x,y: '_'.join([x,y]), [subj_id]*len(subj_pair_ch), subj_pair_ch))\n",
    "        \n",
    "        # compute pwise coherence \n",
    "        pwise = oscillation_utils.compute_connectivity(subj_epochs, \n",
    "                                           band = freq_dict[band], \n",
    "                                           metric = metric, \n",
    "                                           indices = seed_to_target, \n",
    "                                           freqs = freqs, \n",
    "                                           n_cycles = n_cycles,\n",
    "                                           buf_ms = buf_ms, \n",
    "                                           n_surr=n_surr,\n",
    "                                           avg_over_dim='time',\n",
    "                                           band1 = freq_dict[band],\n",
    "                                           parallelize=True)\n",
    "\n",
    "        \n",
    "        coh_df = pd.concat([pd.DataFrame({'epoch':np.arange(0,pwise.shape[0]),'coh':pwise[:,ch_ix],\n",
    "                                          'unique_ch_pair':[ch_name]*pwise.shape[0],\n",
    "                                          'roi_pair_chans':['_'.join(ch_name.split('_')[1:])]*pwise.shape[0],\n",
    "                                          'roi1_ch_names':[ch_name.split('_')[1]]*pwise.shape[0],\n",
    "                                          'roi2_ch_names':[ch_name.split('_')[2]]*pwise.shape[0],\n",
    "                                          'roi1_elec_idx':[seed_to_target[0]]*pwise.shape[0],\n",
    "                                          'roi2_elec_idx':[seed_to_target[1]]*pwise.shape[0]}) \n",
    "                            for ch_ix, ch_name in enumerate(unique_ch_pair)])\n",
    "\n",
    "\n",
    "\n",
    "        coh_df['subj_id']  = subj_id\n",
    "        coh_df['bdi']      = all_behav[all_behav.subj_id == subj_id].bdi.unique().tolist()[0]\n",
    "        coh_df['Round']    = all_behav[all_behav.subj_id == subj_id].Round\n",
    "        coh_df['epoch']    = all_behav[all_behav.subj_id == subj_id].epoch\n",
    "        coh_df['band']     = band\n",
    "        coh_df['metric']   = metric\n",
    "        coh_df['pair_id']  = pair_id\n",
    "        coh_df['roi1']     = source_region\n",
    "        coh_df['roi2']     = target_region\n",
    "\n",
    "\n",
    "        # one pair one subj data \n",
    "        coh_df.to_csv(f'{save_dir}single_subj/{subj_id}_{pair_id}_{metric}_{band}_df.csv')\n",
    "        all_subj_pair_df.append(coh_df)\n",
    "        del coh_df, subj_epochs, pwise\n",
    "#         print(f'finished {subj_id} {pair_id}')\n",
    "    \n",
    "    \n",
    "    # # add completed subj dfs to concat list \n",
    "    if pair in incomplete_pairs:\n",
    "        for subj_id in complete_subj:\n",
    "            subj_df = pd.read_csv(f'{save_dir}single_subj/{subj_id}_{pair_id}_{metric}_{band}_df.csv')\n",
    "            all_subj_pair_df.append(subj_df)\n",
    "            del subj_df   \n",
    "        \n",
    "    all_subj_pair_df = pd.concat(all_subj_pair_df).reset_index()\n",
    "    # save roi pair df separately \n",
    "    all_subj_pair_df.to_csv(f'{save_dir}pair_data/{pair_id}_{metric}_{band}_df.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaca4c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a5689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swb_lfp",
   "language": "python",
   "name": "swb_lfp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
