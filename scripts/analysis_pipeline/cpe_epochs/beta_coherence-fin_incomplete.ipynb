{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63c15ef6",
   "metadata": {},
   "source": [
    "# SWB Connectivity Analysis: Beta Coherence\n",
    "\n",
    "Created: 08/22/2024 \\\n",
    "Updated: 11/06/2024 \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82b06ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore, linregress, ttest_ind, ttest_rel, ttest_1samp, pearsonr, spearmanr\n",
    "import pandas as pd\n",
    "from mne.preprocessing.bads import _find_outliers\n",
    "import os \n",
    "import joblib\n",
    "import re\n",
    "import datetime\n",
    "import scipy\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.regression.mixed_linear_model import MixedLM \n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "import itertools\n",
    "import time \n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "\n",
    "\n",
    "import mne_connectivity\n",
    "from mne_connectivity import phase_slope_index, seed_target_indices, spectral_connectivity_epochs\n",
    "# import fooof\n",
    "# Import plotting functions\n",
    "# from fooof.plts.spectra import plot_spectrum, plot_spectra\n",
    "# # Import the FOOOF object\n",
    "# from fooof import FOOOF\n",
    "# from fooof import FOOOFGroup\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from joblib import delayed, Parallel\n",
    "from statsmodels.stats import multitest\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# print('\\n'.join(f'{m.__name__}=={m.__version__}' for m in globals().values() if getattr(m, '__version__', None)))\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e13b8c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11062024\n"
     ]
    }
   ],
   "source": [
    "# Specify root directory for un-archived data and results \n",
    "base_dir   = '/sc/arion/projects/guLab/Alie/SWB/'\n",
    "neural_dir = f'{base_dir}ephys_analysis/data/'\n",
    "behav_dir  = f'{base_dir}ephys_analysis/behav/behav_data/'\n",
    "save_dir   = f'{base_dir}ephys_analysis/results/connectivity/coherence/beta/'\n",
    "os.makedirs(save_dir,exist_ok=True)\n",
    "\n",
    "script_dir = '/hpc/users/finka03/swb_ephys_analysis/scripts/'\n",
    "\n",
    "date = datetime.date.today().strftime('%m%d%Y')\n",
    "print(date)\n",
    "\n",
    "# anat_dir   = f'{base_dir}ephys_analysis/recon_labels/'\n",
    "# behav_dir  = f'{base_dir}swb_behav_models/data/behavior_preprocessed/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ebfd527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(f'{base_dir}ephys_analysis/LFPAnalysis/')\n",
    "\n",
    "from LFPAnalysis import analysis_utils,oscillation_utils\n",
    "\n",
    "sys.path.append(f'{script_dir}analysis_notebooks/')\n",
    "\n",
    "from ieeg_tools import *\n",
    "\n",
    "sys.path.append(f'{script_dir}behav/')\n",
    "\n",
    "from behav_utils import *\n",
    "from swb_subj_behav import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9316f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_ids = list(pd.read_excel(f'{base_dir}ephys_analysis/subj_info/SWB_subjects.xlsx', \n",
    "                              sheet_name='Usable_Subjects', usecols=[0]).PatientID)\n",
    "n_subj = len(subj_ids)\n",
    "# subj_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fb5f14-a38b-477e-8ebd-5f0b67a60f96",
   "metadata": {},
   "source": [
    "# Load Behav + Elec ROI Data\n",
    "- all_behav from updated task_dfs and behav_utils formatting\n",
    "- roi_reref_labels same as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30605ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # all_behav = pd.read_csv(f'{behav_dir}all_behav.csv') ## this isn't normalized yet \n",
    "# raw_behav = [pd.read_csv(f'{behav_dir}{subj_id}_task_df.csv') for subj_id in subj_ids]\n",
    "# all_behav,drops_data = format_all_behav(raw_behav,return_drops=True)\n",
    "\n",
    "# all_behav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f084ea3-6500-4db6-9bba-27fe414d53d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_behav = [pd.read_csv(f'{behav_dir}{subj_id}_task_df.csv') for subj_id in subj_ids]\n",
    "temp_behav,beh_drops = format_all_behav(raw_behav,return_drops=True,norm=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edef87c9-066d-4028-83ec-05fcf2157d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_orthog_rpe  = []\n",
    "all_rpe_cpe_rsq = []\n",
    "behav_list = []\n",
    "\n",
    "for ix, subj_id in enumerate(subj_ids): \n",
    "#     subj_df = raw_behav[ix]\n",
    "    subj_df = temp_behav[temp_behav.subj_id==subj_id].reset_index(drop=True)\n",
    "    # find indices where cpe is not nan\n",
    "    notnan_idx = [ix for ix,cpe in enumerate(subj_df.cpe) if not np.isnan(cpe)]\n",
    "    # initialize save vector\n",
    "    subj_df['orthog_rpe'] = np.nan\n",
    "    # run ols regression rpe ~ cpe\n",
    "    rpe_cpe = sm.OLS(subj_df.rpe,sm.add_constant(subj_df.cpe),missing='drop').fit()\n",
    "    all_rpe_cpe_rsq.append(rpe_cpe.rsquared)\n",
    "    subj_df['orthog_rpe'][notnan_idx] = rpe_cpe.resid\n",
    "    all_orthog_rpe.extend(subj_df['orthog_rpe'])\n",
    "    behav_list.append(subj_df)\n",
    "    del rpe_cpe \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a4e5ed8-fca0-44df-806d-15339fc7a3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_behav,beh_drops = format_all_behav(behav_list,return_drops=True,norm=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef190f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_reref_labels_master_df = pd.read_csv(\n",
    "    glob(f'{base_dir}ephys_analysis/results/roi_info/roi_reref_labels_master.csv')[0]).drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# roi_reref_labels_master_df #= roi_reref_labels_master_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165accbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### update all subjects epochs data and resave \n",
    "# epoch_id = 'CpeOnset'\n",
    "# # iterate through subjects\n",
    "# for subj_id in subj_ids:\n",
    "#     print(subj_id)\n",
    "#     # load & format rereferenced epoch data \n",
    "#     subj_epochs = mne.read_epochs(f'{neural_dir}{subj_id}/{epoch_id}_epochs.fif', preload=True)\n",
    "#     # drop bad trials\n",
    "#     subj_drops = drops_data[subj_id]\n",
    "#     subj_epochs.drop(subj_drops)\n",
    "#     # replace old metadata with updated subject data\n",
    "#     subj_epochs.metadata = all_behav[all_behav.subj_id == subj_id]\n",
    "#     # save updated epochs data\n",
    "#     subj_epochs.save(f'{neural_dir}{subj_id}/{epoch_id}_epochs-clean.fif', overwrite=True)\n",
    "#     del subj_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d880360",
   "metadata": {},
   "source": [
    "# Connectivity Computations : Beta Coherence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc3d7b51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define connectivity analysis parameters:\n",
    "\n",
    "# spectral parameters - wavelet freqs, wavelet cycles, freq band ranges\n",
    "freqs = np.logspace(*np.log10([2, 200]), num=30)\n",
    "n_cycles = np.floor(np.logspace(*np.log10([3, 10]), num=30))\n",
    "\n",
    "freq_dict = {'theta':[4, 8], \n",
    "             'alpha':[8, 13],\n",
    "             'beta': [13, 30], \n",
    "             'gamma': [30,70],\n",
    "             'hfa': [70, 200]}\n",
    "\n",
    "# analysis parameters - connectivity metric, conn freq band, num of surrogates for permutations, buffer time in ms\n",
    "metric   = 'coh'\n",
    "band     = 'beta' # set band(s) of interest for analysis\n",
    "n_surr   = 500\n",
    "buf_ms   = 1000\n",
    "\n",
    "# data info - analysis epoch + rois for pairwise coh\n",
    "epoch_id = 'CpeOnset' \n",
    "# rois\n",
    "coh_rois = ['acc','ains','ofc','dlpfc','vlpfc','amy','dmpfc']\n",
    "\n",
    "# dict of subj_ids with elecs in roi \n",
    "roi_subj_ids = {f'{roi}':roi_reref_labels_master_df.subj_id[\n",
    "    roi_reref_labels_master_df.roi == roi].unique().tolist() \n",
    "                for roi in coh_rois}\n",
    "# make unique list of pairs [[pair1,pair2],..] without hard coding \n",
    "# pairs = [list(tup) for tup in list(itertools.combinations(coh_rois,2))]\n",
    "# pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d4d7965-6e22-4f9f-80f0-f5bc5964be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combos = [list(tup) for tup in list(itertools.combinations(coh_rois,2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80da8688-daf5-4ebc-8a68-752235d4c363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ofc', 'vlpfc'],\n",
       " ['acc', 'dlpfc'],\n",
       " ['acc', 'ains'],\n",
       " ['ains', 'vlpfc'],\n",
       " ['ains', 'ofc'],\n",
       " ['ains', 'dlpfc'],\n",
       " ['ofc', 'dlpfc'],\n",
       " ['acc', 'ofc'],\n",
       " ['ofc', 'dmpfc'],\n",
       " ['ains', 'dmpfc'],\n",
       " ['acc', 'vlpfc'],\n",
       " ['ains', 'amy'],\n",
       " ['ofc', 'amy'],\n",
       " ['acc', 'amy']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finished_pairs = [file.split('_')[:2] for file in os.listdir(f'{save_dir}pair_data/')]\n",
    "finished_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d59f427-6f74-42bb-89bc-24df66b1c034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['acc', 'dmpfc'], ['vlpfc', 'amy']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## unique list of all pairs that have any single subjects saved \n",
    "all_pair_files = [file.split('_')[1:3] for file in os.listdir(f'{save_dir}single_subj/') if file.split('_')[-1] == 'df.csv']\n",
    "all_pair_files\n",
    "\n",
    "pair_subj_data = []\n",
    "for pair in all_pair_files:\n",
    "    if pair not in pair_subj_data:\n",
    "        pair_subj_data.append(pair)\n",
    "pair_subj_data\n",
    "\n",
    "# # find which pairs weren't completed \n",
    "incomplete_pairs = [pair for pair in pair_subj_data if pair not in finished_pairs]\n",
    "incomplete_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7a58273-4359-466e-9fe2-ad835c83d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_subj_dict   = {}\n",
    "incomplete_subj_dict = {}\n",
    "\n",
    "for pair in incomplete_pairs:\n",
    "    source_region = pair[0]\n",
    "    target_region = pair[1]    \n",
    "    # unique pair id roi1_roi2\n",
    "    pair_id = '_'.join([source_region,target_region])\n",
    "\n",
    "    # find subj with elecs in each roi \n",
    "    source_subj = roi_subj_ids[source_region]\n",
    "    target_subj = roi_subj_ids[target_region]\n",
    "    \n",
    "    # # find subj with elecs in both rois\n",
    "    pair_subj = list(set(source_subj).intersection(target_subj))    \n",
    "    # # find completed subj files\n",
    "    complete_subj_files = glob(f'{save_dir}single_subj/*{pair_id}_coh_beta_df.csv')\n",
    "    complete_subj = [file.split('/')[-1].split('_')[0] for file in complete_subj_files]\n",
    "    complete_subj_dict[pair_id] = complete_subj\n",
    "\n",
    "    # # find subj from pair_subj with no saved data\n",
    "    incomplete_subj = list(np.setdiff1d(pair_subj, complete_subj))\n",
    "    incomplete_subj_dict[pair_id] = incomplete_subj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8ae5024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_subj_pair_df = []\n",
    "# for subj_id in complete_subj:\n",
    "#     subj_df = pd.read_csv(f'{save_dir}single_subj/{subj_id}_{pair_id}_coh_beta_df.csv')\n",
    "#     all_subj_pair_df.append(subj_df)\n",
    "#     del subj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9a2c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /sc/arion/projects/guLab/Alie/SWB/ephys_analysis/data/DA023/DA023_conn_epochs_CpeOnset.fif ...\n",
      "    Found the data of interest:\n",
      "        t =   -1000.00 ...    2000.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "150 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "   Processing epoch 150 / 150 ...\n",
      "[Connectivity computation done]\n"
     ]
    }
   ],
   "source": [
    "######## to compute coh after notebook ends in middle of roi pair\n",
    "\n",
    "for pair in incomplete_pairs:\n",
    "    \n",
    "    source_region = pair[0]\n",
    "    target_region = pair[1]    \n",
    "    \n",
    "    # unique pair id roi1_roi2\n",
    "    pair_id = '_'.join([source_region,target_region])\n",
    "    \n",
    "    # find subj with elecs in each roi \n",
    "    source_subj = roi_subj_ids[source_region]\n",
    "    target_subj = roi_subj_ids[target_region]\n",
    "    # find subj with elecs in both rois\n",
    "    pair_subj = list(set(source_subj).intersection(target_subj))    \n",
    "    complete_subj   = complete_subj_dict[pair_id]\n",
    "    incomplete_subj = incomplete_subj_dict[pair_id]\n",
    "    \n",
    "    \n",
    "    # initialize the storage list \n",
    "    all_subj_pair_df = []\n",
    "    # run incomplete subj only! \n",
    "    for subj_id in incomplete_subj:\n",
    "    \n",
    "        # load & format rereferenced epoch data \n",
    "        subj_epochs = mne.read_epochs(f'{neural_dir}{subj_id}/{subj_id}_conn_epochs_{epoch_id}.fif', preload=True)\n",
    "        subj_elecs  = subj_epochs.ch_names\n",
    "    \n",
    "        # construct the seed-to-target mapping based on subject's roi coverage \n",
    "        elec_roi_df = roi_reref_labels_master_df[roi_reref_labels_master_df.subj_id==subj_id].reset_index(drop=True)\n",
    "        # get ch names of subj elecs in roi \n",
    "        source_ch_names  = elec_roi_df.reref_ch_names[np.where(elec_roi_df.roi == source_region)[0]].tolist()\n",
    "        target_ch_names  = elec_roi_df.reref_ch_names[np.where(elec_roi_df.roi == target_region)[0]].tolist()\n",
    "        # get idx of ch in subj_elecs list (will correspond to idx in epochs array)\n",
    "        source_elec_idx = [subj_elecs.index(elec) for elec in source_ch_names]\n",
    "        target_elec_idx = [subj_elecs.index(elec) for elec in target_ch_names]\n",
    "        # make seed to target indices using mne function \n",
    "        seed_to_target = seed_target_indices(\n",
    "                        source_elec_idx,\n",
    "                        target_elec_idx)\n",
    "    \n",
    "        # elec name for every elec pair \n",
    "        subj_pair_ch = list(map(lambda x,y: '_'.join([x,y]), \n",
    "                                  [subj_elecs[idx] for idx in  seed_to_target[0]], \n",
    "                                  [subj_elecs[idx] for idx in  seed_to_target[1]]))\n",
    "        # unique elec name for every elec pair \n",
    "        unique_ch_pair = list(map(lambda x,y: '_'.join([x,y]), [subj_id]*len(subj_pair_ch), subj_pair_ch))\n",
    "    \n",
    "        # compute pwise coherence \n",
    "        pwise = oscillation_utils.compute_connectivity(subj_epochs.copy(), \n",
    "                                           band = freq_dict[band], \n",
    "                                           metric = metric, \n",
    "                                           indices = seed_to_target, \n",
    "                                           freqs = freqs, \n",
    "                                           n_cycles = n_cycles,\n",
    "                                           buf_ms = buf_ms, \n",
    "                                           n_surr=n_surr,\n",
    "                                           avg_over_dim='time',\n",
    "                                           band1 = freq_dict[band],\n",
    "                                           parallelize=True)\n",
    "    \n",
    "    \n",
    "        coh_df = pd.concat([pd.DataFrame({'epoch':np.arange(0,pwise.shape[0]),'coh':pwise[:,ch_ix],\n",
    "                                          'unique_ch_pair':[ch_name]*pwise.shape[0],\n",
    "                                          'roi_pair_chans':['_'.join(ch_name.split('_')[1:])]*pwise.shape[0],\n",
    "                                          'roi1_ch_names':[ch_name.split('_')[1]]*pwise.shape[0],\n",
    "                                          'roi2_ch_names':[ch_name.split('_')[2]]*pwise.shape[0],\n",
    "                                          'roi1_elec_idx':[seed_to_target[0]]*pwise.shape[0],\n",
    "                                          'roi2_elec_idx':[seed_to_target[1]]*pwise.shape[0]}) \n",
    "                            for ch_ix, ch_name in enumerate(unique_ch_pair)])\n",
    "    \n",
    "    \n",
    "    \n",
    "        coh_df['subj_id']  = subj_id\n",
    "        coh_df['bdi']      = all_behav[all_behav.subj_id == subj_id].bdi.unique().tolist()[0]\n",
    "        coh_df['Round']    = all_behav[all_behav.subj_id == subj_id].Round\n",
    "        coh_df['epoch']    = all_behav[all_behav.subj_id == subj_id].epoch\n",
    "        coh_df['band']     = band\n",
    "        coh_df['metric']   = metric\n",
    "        coh_df['pair_id']  = pair_id\n",
    "        coh_df['roi1']     = source_region\n",
    "        coh_df['roi2']     = target_region\n",
    "    \n",
    "        # one pair one subj data \n",
    "        coh_df.to_csv(f'{save_dir}{subj_id}_{pair_id}_{metric}_{band}_df.csv')\n",
    "        all_subj_pair_df.append(coh_df)\n",
    "        del coh_df, subj_epochs,pwise\n",
    "    \n",
    "    # # add completed subj dfs to concat list \n",
    "    for subj_id in complete_subj:\n",
    "        subj_df = pd.read_csv(f'{save_dir}single_subj/{subj_id}_{pair_id}_coh_beta_df.csv')\n",
    "        all_subj_pair_df.append(subj_df)\n",
    "        del subj_df   \n",
    "        \n",
    "    # # make pair df \n",
    "    all_subj_pair_df = pd.concat(all_subj_pair_df).reset_index()\n",
    "    # # save roi pair df  \n",
    "    all_subj_pair_df.to_csv(f'{save_dir}{pair_id}_{metric}_{band}_df.csv', index=False)\n",
    "\n",
    "print('all done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809bd0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for pair in pairs:\n",
    "    \n",
    "#     source_region = pair[0]\n",
    "#     target_region = pair[1]    \n",
    "    \n",
    "#     # unique pair id roi1_roi2\n",
    "#     pair_id = '_'.join([source_region,target_region])\n",
    "    \n",
    "#     # find subj with elecs in each roi \n",
    "#     source_subj = roi_subj_ids[source_region]\n",
    "#     target_subj = roi_subj_ids[target_region]\n",
    "#     # find subj with elecs in both rois\n",
    "#     pair_subj = list(set(source_subj).intersection(target_subj))    \n",
    "#     # save pair subj list \n",
    "    \n",
    "#     # initialize the storage list \n",
    "#     all_subj_pair_df = []\n",
    "    \n",
    "#     # iterate through pair subjects\n",
    "#     for subj_id in pair_subj:\n",
    "        \n",
    "#         # load & format rereferenced epoch data \n",
    "#         subj_epochs = mne.read_epochs(f'{neural_dir}{subj_id}/{subj_id}_conn_epochs_{epoch_id}.fif', preload=True)\n",
    "#         subj_elecs  = subj_epochs.ch_names\n",
    "        \n",
    "#         # construct the seed-to-target mapping based on subject's roi coverage \n",
    "#         elec_roi_df = roi_reref_labels_master_df[roi_reref_labels_master_df.subj_id==subj_id].reset_index(drop=True)\n",
    "#         # get ch names of subj elecs in roi \n",
    "#         source_ch_names  = elec_roi_df.reref_ch_names[np.where(elec_roi_df.roi == source_region)[0]].tolist()\n",
    "#         target_ch_names  = elec_roi_df.reref_ch_names[np.where(elec_roi_df.roi == target_region)[0]].tolist()\n",
    "#         # get idx of ch in subj_elecs list (will correspond to idx in epochs array)\n",
    "#         source_elec_idx = [subj_elecs.index(elec) for elec in source_ch_names]\n",
    "#         target_elec_idx = [subj_elecs.index(elec) for elec in target_ch_names]\n",
    "#         # make seed to target indices using mne function \n",
    "#         seed_to_target = seed_target_indices(\n",
    "#                         source_elec_idx,\n",
    "#                         target_elec_idx)\n",
    "        \n",
    "#         # elec name for every elec pair \n",
    "#         subj_pair_ch = list(map(lambda x,y: '_'.join([x,y]), \n",
    "#                                   [subj_elecs[idx] for idx in  seed_to_target[0]], \n",
    "#                                   [subj_elecs[idx] for idx in  seed_to_target[1]]))\n",
    "#         # unique elec name for every elec pair \n",
    "#         unique_ch_pair = list(map(lambda x,y: '_'.join([x,y]), [subj_id]*len(subj_pair_ch), subj_pair_ch))\n",
    "        \n",
    "#         # compute pwise coherence \n",
    "#         pwise = oscillation_utils.compute_connectivity(subj_epochs, \n",
    "#                                            band = freq_dict[band], \n",
    "#                                            metric = metric, \n",
    "#                                            indices = seed_to_target, \n",
    "#                                            freqs = freqs, \n",
    "#                                            n_cycles = n_cycles,\n",
    "#                                            buf_ms = buf_ms, \n",
    "#                                            n_surr=n_surr,\n",
    "#                                            avg_over_dim='time',\n",
    "#                                            band1 = freq_dict[band],\n",
    "#                                            parallelize=True)\n",
    "\n",
    "        \n",
    "#         coh_df = pd.concat([pd.DataFrame({'epoch':np.arange(0,pwise.shape[0]),'coh':pwise[:,ch_ix],\n",
    "#                                           'unique_ch_pair':[ch_name]*pwise.shape[0],\n",
    "#                                           'roi_pair_chans':['_'.join(ch_name.split('_')[1:])]*pwise.shape[0],\n",
    "#                                           'roi1_ch_names':[ch_name.split('_')[1]]*pwise.shape[0],\n",
    "#                                           'roi2_ch_names':[ch_name.split('_')[2]]*pwise.shape[0],\n",
    "#                                           'roi1_elec_idx':[seed_to_target[0]]*pwise.shape[0],\n",
    "#                                           'roi2_elec_idx':[seed_to_target[1]]*pwise.shape[0]}) \n",
    "#                             for ch_ix, ch_name in enumerate(unique_ch_pair)])\n",
    "\n",
    "\n",
    "\n",
    "#         coh_df['subj_id']  = subj_id\n",
    "#         coh_df['bdi']      = all_behav[all_behav.subj_id == subj_id].bdi.unique().tolist()[0]\n",
    "#         coh_df['Round']    = all_behav[all_behav.subj_id == subj_id].Round\n",
    "#         coh_df['epoch']    = all_behav[all_behav.subj_id == subj_id].epoch\n",
    "#         coh_df['band']     = band\n",
    "#         coh_df['metric']   = metric\n",
    "#         coh_df['pair_id']  = pair_id\n",
    "#         coh_df['roi1']     = source_region\n",
    "#         coh_df['roi2']     = target_region\n",
    "\n",
    "#         # one pair one subj data \n",
    "#         coh_df.to_csv(f'{save_dir}single_subj/{subj_id}_{pair_id}_{metric}_{band}_df.csv')\n",
    "#         all_subj_pair_df.append(coh_df)\n",
    "#         del coh_df, subj_epochs, pwise\n",
    "# #         print(f'finished {subj_id} {pair_id}')\n",
    "        \n",
    "#     # one pair all subj data \n",
    "# #     print(f'finished all {pair_id}')\n",
    "#     all_subj_pair_df = pd.concat(all_subj_pair_df).reset_index()\n",
    "#     # save roi pair df separately \n",
    "#     all_subj_pair_df.to_csv(f'{save_dir}{pair_id}_{metric}_{band}_df_1sec.csv', index=False)\n",
    "    \n",
    "#     del all_subj_pair_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a5689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roi_labels = pd.read_csv('/Users/alexandrafink/Documents/GraduateSchool/SaezLab/SWB/ephys_analysis/roi_reref_labels_master.csv')\n",
    "# acc_subj   = roi_labels[roi_labels.roi=='acc'].subj_id.unique().tolist()\n",
    "# vlpfc_subj = roi_labels[roi_labels.roi=='vlpfc'].subj_id.unique().tolist()\n",
    "# pair_subj = list(set(acc_subj).intersection(vlpfc_subj)) \n",
    "# pair_subj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swb_lfp",
   "language": "python",
   "name": "swb_lfp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
