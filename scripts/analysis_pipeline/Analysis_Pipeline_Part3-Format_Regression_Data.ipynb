{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWB analysis pipeline part 2: \n",
    "\n",
    "- aggregate CPE epoch power data + metadata + roi locs \n",
    "- make dataframes for trial averaged regressions for hfa + beta \n",
    "- format time-resolved tfr data for TFR x condition plots \n",
    "- concat tfr ROI data across subjects \n",
    "\n",
    "\n",
    "*Created: 04/08/24* \\\n",
    "*Updated: 06/24/2024*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "from scipy.stats import zscore, linregress, ttest_ind, ttest_rel, ttest_1samp, pearsonr, spearmanr\n",
    "import pandas as pd\n",
    "from mne.preprocessing.bads import _find_outliers\n",
    "import os \n",
    "import joblib\n",
    "import re\n",
    "import datetime\n",
    "import scipy\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.regression.mixed_linear_model import MixedLM \n",
    "from joblib import Parallel, delayed\n",
    "import pickle\n",
    "import itertools\n",
    "import time \n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify root directory for un-archived data and results \n",
    "base_dir   = '/sc/arion/projects/guLab/Alie/SWB/'\n",
    "anat_dir   = f'{base_dir}ephys_analysis/recon_labels/'\n",
    "neural_dir = f'{base_dir}ephys_analysis/data/'\n",
    "behav_dir  = f'{base_dir}swb_behav_models/data/behavior_preprocessed/'\n",
    "save_dir   = f'{base_dir}ephys_analysis/results/regression_data/'\n",
    "script_dir = '/hpc/users/finka03/swb_ephys_analysis/scripts/'\n",
    "\n",
    "date = datetime.date.today().strftime('%m%d%Y')\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(f'{base_dir}ephys_analysis/LFPAnalysis/')\n",
    "\n",
    "from LFPAnalysis import analysis_utils\n",
    "\n",
    "sys.path.append(f'{script_dir}analysis_notebooks/')\n",
    "\n",
    "from ieeg_tools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_ids = list(pd.read_excel(f'{base_dir}ephys_analysis/subj_info/SWB_subjects.xlsx', sheet_name='Usable_Subjects', usecols=[0]).PatientID)\n",
    "n_subj = len(subj_ids)\n",
    "# subj_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bdi_list = pd.read_excel(f'{base_dir}ephys_analysis/subj_info/SWB_subjects.xlsx', sheet_name='Usable_Subjects', usecols=[3])\n",
    "bdi_list = list(bdi_list.SWB_BDI)\n",
    "subj_info_df = pd.DataFrame({'subj_id':subj_ids,'bdi':bdi_list})\n",
    "subj_info_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analysis_freqs = {'delta':[2,4],'theta':[4,9],'alpha':[9,13],'beta':[13,30],'gamma':[30,70],'hfa':[70,200]}\n",
    "analysis_freqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subj_power     = {}\n",
    "all_subj_metadata  = {}\n",
    "\n",
    "\n",
    "# load pow epochs data \n",
    "for subj_id in subj_ids: \n",
    "    \n",
    "    # load cpe power epochs for single subject \n",
    "    power_epochs = mne.time_frequency.read_tfrs(f'{neural_dir}{subj_id}/{subj_id}_CpeOnset-tfr.h5')[0]\n",
    "    all_subj_metadata[subj_id] = power_epochs.metadata.copy()\n",
    "\n",
    "    # convert mne power data to data frame\n",
    "    power_df = power_epochs.to_data_frame()\n",
    "    del power_epochs # remove power epochs from memory to speed up loop\n",
    "    \n",
    "    subj_band_dfs = {}\n",
    "    # extract power df with mean freq-specific power for each epoch for each elec \n",
    "    for band,freq_range in analysis_freqs.items():\n",
    "        band_df = power_df[(power_df.freq >= freq_range[0])&(power_df.freq <= freq_range[1])].groupby(['epoch']).agg('mean').reset_index()\n",
    "        band_df['band'] = band\n",
    "        subj_band_dfs[band] = band_df \n",
    "    del power_df # delete full power df to save memory\n",
    "    all_subj_power[subj_id] = subj_band_dfs\n",
    "    \n",
    "# create a binary pickle file \n",
    "pickle.dump(all_subj_power,\n",
    "            open(f'{save_dir}all_subj_power_{(\"_\").join(list(analysis_freqs.keys()))}_{date}.pkl',\"wb\"))\n",
    "\n",
    "# # # create a binary pickle file \n",
    "pickle.dump(all_subj_metadata,\n",
    "            open(f'{save_dir}all_subj_metadata.pkl',\"wb\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Regression DFs\n",
    "\n",
    "1. Update metadata vars - add t+1 vars, normalize vecs by 2std\n",
    "2. Aggregate metadata, power data, ROI info, bdi info into one master df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### update metadata - zscore continuous regressors + add RT info\n",
    "for subj_id in subj_ids:\n",
    "    subj_df         = all_subj_metadata[subj_id]\n",
    "    rounds_t1       = subj_df.Round.tolist()[1:]\n",
    "    logRT_list      = np.log(subj_df['RT']).tolist() # take log of RT\n",
    "    logRT_t1_list   = logRT_list[1:] # get logRT_t1\n",
    "    GambleChoice_t1 = subj_df.GambleChoice.tolist()[1:]\n",
    "    GambleEV_t1     = subj_df.GambleEV.tolist()[1:]\n",
    "    TrialType_t1    = subj_df.TrialType.tolist()[1:]\n",
    "    TrialEV         = subj_df.GambleEV - subj_df.SafeBet\n",
    "    TrialEV_t1      = TrialEV[1:].tolist()\n",
    "    rounds_t1.append(np.nan)\n",
    "    logRT_t1_list.append(np.nan) # add nan to logRT_t1 (no t1 for last round)\n",
    "    GambleChoice_t1.append(np.nan)\n",
    "    GambleEV_t1.append(np.nan)\n",
    "    TrialType_t1.append(np.nan) \n",
    "    TrialEV_t1.append(np.nan)\n",
    "    subj_df['logRT']           = logRT_list\n",
    "    subj_df['logRT_t1']        = logRT_t1_list\n",
    "    subj_df['Round_t1']        = rounds_t1\n",
    "    subj_df['GambleChoice_t1'] = GambleChoice_t1\n",
    "    subj_df['GambleEV_t1']     = GambleEV_t1\n",
    "    subj_df['TrialType_t1']    = TrialType_t1\n",
    "    subj_df['TrialEV']         = TrialEV\n",
    "    subj_df['TrialEV_t1']      = TrialEV_t1\n",
    "    subj_df['bdi']             = subj_info_df.bdi[subj_info_df.subj_id==subj_id].values[0]\n",
    "    subj_df['subj_id']         = subj_id\n",
    "    \n",
    "    # update metadata\n",
    "    all_subj_metadata[subj_id] = subj_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # create a binary pickle file \n",
    "pickle.dump(all_subj_metadata,\n",
    "            open(f'{save_dir}all_subj_metadata_clean.pkl',\"wb\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# round_test = [list(zip(all_subj_metadata[key].Round.tolist(),all_subj_metadata[key].RT.tolist(),all_subj_metadata[key].logRT.tolist(),all_subj_metadata[key].logRT_t1.tolist())) for key in all_subj_metadata.keys()]\n",
    "# is_consecutive = [all([all_subj_metadata[subj_id].Round[i] == all_subj_metadata[subj_id].Round_t1[i-1] for i in range(1, len(all_subj_metadata[subj_id].Round))]) for subj_id in all_subj_metadata.keys()]\n",
    "# is_consecutive\n",
    "# round_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load ROI reref master \n",
    "roi_reref_labels_master_df = pd.read_csv(glob(f'{base_dir}ephys_analysis/results/roi_info/roi_reref_labels_master.csv')[0])\n",
    "roi_reref_labels_master_df = roi_reref_labels_master_df.drop(columns=['Unnamed: 0'])\n",
    "roi_reref_labels_master_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_band_data = {f'{freq}':[] for freq in list(analysis_freqs.keys())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_regressors = ['TrialEV','TrialEV_t1','GambleEV','GambleEV_t1','SafeBet',\n",
    "                         'TotalProfit','CR', 'choiceEV', 'RPE','Profit',\n",
    "                         'decisionCPE', 'decisionRegret','decisionRelief']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for band in list(analysis_freqs.keys()):\n",
    "    \n",
    "    \n",
    "    band_master_df = []\n",
    "    \n",
    "    for subj_id in subj_ids: \n",
    "        pow_df = all_subj_power[subj_id][band].copy()\n",
    "\n",
    "        # reshape pow_df \n",
    "        pow_reg_df = pow_df.melt(id_vars=['epoch','freq','band'],\n",
    "                                    value_vars=pow_df.columns.drop(['epoch','time','freq']).tolist().copy(),\n",
    "                                    var_name='reref_ch_names', value_name='band_pow',\n",
    "                                    ignore_index = False)\n",
    "\n",
    "        pow_reg_df['subj_id'] = subj_id # add subj_id to df values\n",
    "        # reshape metadata df \n",
    "        metadata_df = all_subj_metadata[subj_id].reset_index()\n",
    "        metadata_df = metadata_df.rename(columns={'index':'epoch'})\n",
    "\n",
    "        # merge pow and metadata dfs keeping epoch as index \n",
    "        subj_regression_df = pd.merge(pow_reg_df, metadata_df,left_on=['epoch','subj_id'],\n",
    "                                    right_on=['epoch','subj_id']).reset_index(drop=True).set_index('epoch')\n",
    "        subj_regression_df['epoch_num'] = subj_regression_df.index # keep epoch as a column\n",
    "\n",
    "        # add roi info \n",
    "        subj_epochs = pow_reg_df.epoch.unique()\n",
    "        # extract roi_reref_labels_df for each subj \n",
    "        subj_roi_reref_labels = roi_reref_labels_master_df[roi_reref_labels_master_df['subj_id']==subj_id]\n",
    "        # add epochs to df - repeat each elec id for every epoch to join with master df \n",
    "        subj_roi_reref_labels = (pd.concat([subj_roi_reref_labels]*len(subj_epochs),\n",
    "                   keys = subj_epochs,\n",
    "                   names = ['epoch',None])).reset_index(level=1,drop=True) #make sure epochs are the index\n",
    "        subj_roi_reref_labels['epoch_num'] = subj_roi_reref_labels.index\n",
    "\n",
    "        # create unique reref ch id for regression\n",
    "        subj_roi_reref_labels['unique_reref_ch'] = subj_roi_reref_labels[['subj_id', 'reref_ch_names']].agg('_'.join, axis=1)\n",
    "\n",
    "        # merge regression df with roi reref info df  \n",
    "        subj_regression_df = pd.merge(subj_regression_df, subj_roi_reref_labels,left_on=\n",
    "                                      ['subj_id','reref_ch_names','epoch_num'],right_on=\n",
    "                                      ['subj_id','reref_ch_names','epoch_num']).reset_index(\n",
    "                                        drop=True).set_index('epoch_num')\n",
    "        \n",
    "        subj_regression_df['epoch'] = subj_regression_df.index\n",
    "        \n",
    "\n",
    "        ### cleaning within subject BEFORE zscoring regressors! \n",
    "        # remove trials where RT is too fast (random choice)\n",
    "        subj_regression_df = subj_regression_df[subj_regression_df.RT>0.3].reset_index(drop=True)\n",
    "        # remove trials where there is no choice at trial t\n",
    "        subj_regression_df = subj_regression_df[(subj_regression_df.GambleChoice=='gamble') | \n",
    "                                                (subj_regression_df.GambleChoice == 'safe')].reset_index(drop=True)\n",
    "        # remove trials where there is no choice for trial t1 (RT_t1 is nonsense)\n",
    "        subj_regression_df = subj_regression_df[(subj_regression_df.GambleChoice_t1=='gamble') | \n",
    "                                                (subj_regression_df.GambleChoice_t1 == 'safe')].reset_index(drop=True)\n",
    "        # remove trials where there is no outcome\n",
    "        subj_regression_df = subj_regression_df[(subj_regression_df.Outcome=='good') | \n",
    "                                                (subj_regression_df.Outcome == 'bad')].reset_index(drop=True)\n",
    "        \n",
    "        # remove epoch 76 bc photodiode times are not correct at breakpoint \n",
    "        subj_regression_df = subj_regression_df[subj_regression_df.Round != 76].reset_index(drop=True)\n",
    "        # remove trials where logRT_t1 is inf or -inf (means RT = 0)\n",
    "        subj_regression_df = subj_regression_df[~np.isinf(subj_regression_df.logRT_t1)].reset_index(drop=True)\n",
    "        # format GambleChoice + TrialType vars as categories \n",
    "        subj_regression_df['GambleChoice']    = subj_regression_df['GambleChoice'].astype('category')\n",
    "        subj_regression_df['GambleChoice_t1'] = subj_regression_df['GambleChoice_t1'].astype('category')\n",
    "        subj_regression_df['TrialType']       = subj_regression_df['TrialType'].astype('category')\n",
    "        subj_regression_df['TrialType_t1']    = subj_regression_df['TrialType_t1'].astype('category')\n",
    "        # make sure Round_t1 is an integer\n",
    "        subj_regression_df['Round_t1']        = subj_regression_df['Round_t1'].astype('Int64')\n",
    "        \n",
    "        # zscore continuous regressors AFTER removing bad trials!! \n",
    "        for reg in continuous_regressors:\n",
    "            subj_regression_df[reg] = norm_zscore(subj_regression_df[reg].values) # zscore is /2std\n",
    "\n",
    "\n",
    "        band_master_df.append(subj_regression_df)\n",
    "        \n",
    "        del subj_regression_df,pow_df, pow_reg_df,metadata_df,subj_roi_reref_labels\n",
    "\n",
    "    \n",
    "    \n",
    "    band_master_df = pd.concat(band_master_df).reset_index(drop=True)\n",
    "    # reorder columns \n",
    "    band_master_df = band_master_df[['subj_id','bdi','reref_ch_names','unique_reref_ch','epoch','freq','band',\n",
    "                                     'band_pow','roi','hemi','ch_label4roi','ch_type4roi','loc4roi',\n",
    "                                     'logRT_t1','logRT','RT','Round', 'Round_t1','TrialType','TrialType_t1',\n",
    "                                     'GambleEV', 'GambleEV_t1','SafeBet','GambleChoice','GambleChoice_t1','TrialEV',                                 \n",
    "                                     'Profit','TotalProfit','RPE','decisionCPE', 'decisionRegret','decisionRelief']]\n",
    "\n",
    "    band_master_df.to_csv(f'{save_dir}{band}_master_df.csv')\n",
    "    regression_band_data[band] = band_master_df\n",
    "    del band_master_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_band_data['theta']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfp_env",
   "language": "python",
   "name": "lfp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
